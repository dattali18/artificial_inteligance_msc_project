# ============================================================================
# REINFORCEMENT LEARNING NAVIGATION SYSTEM - FINAL PROJECT
# A* vs Q-Learning: A Comparative Study
# ============================================================================

print("Installing required packages...")
try:
    import osmnx as ox
except ImportError:
    !pip install osmnx networkx matplotlib numpy pandas -q
    import osmnx as ox

import networkx as nx
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import random
from collections import defaultdict, deque
import time

print("âœ“ Packages installed successfully!\n")

# ============================================================================
# PART 1: ENVIRONMENT & HELPERS
# ============================================================================

class NavigationEnvironment:
    """
    OpenStreetMap as an RL Environment
    - State: Current node ID
    - Action: Neighbor node ID
    - Reward: Distance heuristic + Step penalty
    """
    
    def __init__(self, graph, start_node, goal_node):
        self.graph = graph
        self.start = start_node
        self.goal = goal_node
        self.current = start_node
        
        # Cache goal coordinates
        self.goal_x = graph.nodes[goal_node]['x']
        self.goal_y = graph.nodes[goal_node]['y']
        
    def reset(self):
        self.current = self.start
        return self.current
    
    def get_distance_to_goal(self, node):
        """Calculates Great Circle distance to goal (Heuristic)"""
        node_x = self.graph.nodes[node]['x']
        node_y = self.graph.nodes[node]['y']
        return ox.distance.great_circle(
            self.graph.nodes[node]['y'], node_x,
            self.goal_y, self.goal_x
        )
    
    def step(self, action):
        next_state = action
        done = False
        
        # Get actual distance traveled (edge length)
        try:
            edge_data = self.graph.get_edge_data(self.current, next_state)[0]
            edge_length = edge_data['length']
        except:
            edge_length = 10 # Fallback
        
        # --- REWARD FUNCTION ---
        if next_state == self.goal:
            reward = 1000  # Terminal reward
            done = True
        else:
            # 1. Potential-based Reward Shaping (Difference in distance to goal)
            # If we got closer, this is positive. If we moved away, this is negative.
            current_dist = self.get_distance_to_goal(self.current)
            next_dist = self.get_distance_to_goal(next_state)
            progress = (current_dist - next_dist)
            
            # 2. Step penalty (cost of movement)
            # We penalize based on edge length to find shortest path, not just any path
            reward = (progress * 5) - (edge_length * 0.1) 
            
            # 3. Dead-end/Loop penalty (optional soft penalty)
            if next_dist > current_dist:
                reward -= 5 

        self.current = next_state
        return next_state, reward, done, {}
    
    def get_valid_actions(self, state):
        return list(self.graph.neighbors(state))

# ============================================================================
# PART 2: ALGORITHMS
# ============================================================================

class AStarSolver:
    """
    A* Algorithm: Theoretical Baseline (Optimal)
    Uses knowledge of the map (coordinates) to plan ahead.
    """
    def __init__(self, graph):
        self.graph = graph
        self.nodes_expanded = 0
    
    def heuristic(self, u, v):
        return ox.distance.great_circle(
            self.graph.nodes[u]['y'], self.graph.nodes[u]['x'], 
            self.graph.nodes[v]['y'], self.graph.nodes[v]['x']
        )
    
    def solve(self, start, goal):
        import heapq
        self.nodes_expanded = 0
        
        # Priority Queue: (f_score, node_id)
        open_set = []
        heapq.heappush(open_set, (0, start))
        
        came_from = {}
        g_score = {node: float('inf') for node in self.graph.nodes}
        g_score[start] = 0
        
        f_score = {node: float('inf') for node in self.graph.nodes}
        f_score[start] = self.heuristic(start, goal)
        
        closed_set = set()
        
        while open_set:
            _, current = heapq.heappop(open_set)
            
            if current == goal:
                return self.reconstruct_path(came_from, current), g_score[goal]
            
            if current in closed_set: continue
            closed_set.add(current)
            self.nodes_expanded += 1
            
            for neighbor in self.graph.neighbors(current):
                edge_len = self.graph.get_edge_data(current, neighbor)[0]['length']
                tentative_g = g_score[current] + edge_len
                
                if tentative_g < g_score[neighbor]:
                    came_from[neighbor] = current
                    g_score[neighbor] = tentative_g
                    f_score[neighbor] = g_score[neighbor] + self.heuristic(neighbor, goal)
                    heapq.heappush(open_set, (f_score[neighbor], neighbor))
                    
        return None, float('inf')

    def reconstruct_path(self, came_from, current):
        path = [current]
        while current in came_from:
            current = came_from[current]
            path.append(current)
        return path[::-1]


class QLearningNavigator:
    """
    Q-Learning: Model-Free RL
    Learns policy table Q(s,a) without knowing map topology initially.
    """
    def __init__(self, env, alpha=0.1, gamma=0.99, epsilon=1.0, decay=0.995):
        self.env = env
        self.alpha = alpha      # Learning Rate
        self.gamma = gamma      # Discount Factor
        self.epsilon = epsilon  # Exploration Rate
        self.decay = decay
        self.min_epsilon = 0.01
        
        # Q-Table: Dictionary of dictionaries
        # state -> {action: value}
        self.q_table = defaultdict(lambda: defaultdict(float))
        self.training_rewards = []

    def get_max_q(self, state):
        actions = self.env.get_valid_actions(state)
        if not actions: return 0.0
        return max([self.q_table[state][a] for a in actions])

    def choose_action(self, state, force_greedy=False):
        actions = self.env.get_valid_actions(state)
        if not actions: return None
        
        # Exploration
        if not force_greedy and random.random() < self.epsilon:
            return random.choice(actions)
            
        # Exploitation
        q_values = [self.q_table[state][a] for a in actions]
        max_q = max(q_values)
        
        # Handle ties randomly to prevent getting stuck
        best_actions = [a for a, q in zip(actions, q_values) if q == max_q]
        return random.choice(best_actions)

    def train(self, episodes, max_steps=200):
        print(f"Training Q-Learning ({episodes} episodes)...")
        
        for ep in range(episodes):
            state = self.env.reset()
            total_reward = 0
            done = False
            steps = 0
            
            while not done and steps < max_steps:
                action = self.choose_action(state)
                if not action: break # Dead end
                
                next_state, reward, done, _ = self.env.step(action)
                
                # --- BELLMAN UPDATE EQUATION ---
                # Q(s,a) = Q(s,a) + alpha * [R + gamma * max(Q(s',a')) - Q(s,a)]
                current_q = self.q_table[state][action]
                max_next_q = self.get_max_q(next_state)
                
                new_q = current_q + self.alpha * (reward + self.gamma * max_next_q - current_q)
                self.q_table[state][action] = new_q
                
                state = next_state
                total_reward += reward
                steps += 1
            
            # Decay exploration
            if self.epsilon > self.min_epsilon:
                self.epsilon *= self.decay
                
            self.training_rewards.append(total_reward)
            
            if (ep+1) % 200 == 0:
                print(f"  Ep {ep+1}: Avg Reward {np.mean(self.training_rewards[-50:]):.1f}, Epsilon {self.epsilon:.2f}")

    def get_path(self):
        """Run the learned policy (greedy)"""
        state = self.env.reset()
        path = [state]
        steps = 0
        done = False
        
        while not done and steps < 100:
            action = self.choose_action(state, force_greedy=True)
            if not action: break
            
            state, _, done, _ = self.env.step(action)
            path.append(state)
            steps += 1
            if done: break
            
        return path

# ============================================================================
# PART 3: MAIN EXECUTION & VISUALIZATION
# ============================================================================

def run_experiment():
    # 1. Setup Map
    print("\n1. Downloading Map Data (Jerusalem)...")
    place = "Ramat Sharet, Jerusalem, Israel"
    G = ox.graph_from_address(place, dist=600, network_type='walk')
    
    # 2. Select Points
    nodes = list(G.nodes)
    start_node = nodes[0] 
    goal_node = nodes[55] # Arbitrary goal
    
    # Calculate straight line distance
    start_c = (G.nodes[start_node]['y'], G.nodes[start_node]['x'])
    goal_c = (G.nodes[goal_node]['y'], G.nodes[goal_node]['x'])
    straight_dist = ox.distance.great_circle(start_c[0], start_c[1], goal_c[0], goal_c[1])
    print(f"Task: Navigate {straight_dist:.1f}m (Straight Line)\n")

    # 3. Run A* (The Baseline)
    print("--- Running A* Algorithm ---")
    astar = AStarSolver(G)
    start_time = time.time()
    astar_path, astar_dist = astar.solve(start_node, goal_node)
    astar_time = time.time() - start_time
    print(f"A* Complete: {len(astar_path)} steps, {astar_dist:.1f}m distance\n")

    # 4. Run Q-Learning
    print("--- Running Q-Learning ---")
    env = NavigationEnvironment(G, start_node, goal_node)
    rl_agent = QLearningNavigator(env, alpha=0.1, gamma=0.95, epsilon=1.0)
    
    start_train = time.time()
    rl_agent.train(episodes=1000)
    train_time = time.time() - start_train
    
    rl_path = rl_agent.get_path()
    
    # Calculate RL path length
    rl_dist = 0
    if rl_path[-1] == goal_node:
        for i in range(len(rl_path)-1):
            d = G.get_edge_data(rl_path[i], rl_path[i+1])[0]['length']
            rl_dist += d
    else:
        rl_dist = 0 # Failed
    
    # 5. Visual Comparison
    compare_results(G, start_node, goal_node, astar_path, rl_path, 
                   {'astar_dist': astar_dist, 'rl_dist': rl_dist, 
                    'astar_time': astar_time, 'rl_time': train_time})

def compare_results(G, start, goal, astar_path, rl_path, metrics):
    """
    Showcase the differences in a better way:
    1. Side-by-side Map
    2. Data Table
    3. Bar Charts
    """
    print("\n" + "="*60)
    print("FINAL COMPARISON: A* vs Q-LEARNING")
    print("="*60)
    
    # --- VISUALIZATION 1: MAPS ---
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))
    
    # Helper to plot
    def plot_on_ax(ax, path, color, title):
        ox.plot_graph(G, ax=ax, show=False, close=False, node_size=0, edge_color='#999999')
        if len(path) > 1:
            # Plot path
            path_nodes_x = [G.nodes[n]['x'] for n in path]
            path_nodes_y = [G.nodes[n]['y'] for n in path]
            ax.plot(path_nodes_x, path_nodes_y, color=color, linewidth=4, alpha=0.8, label='Path')
            
            # Start/Goal
            sx, sy = G.nodes[start]['x'], G.nodes[start]['y']
            gx, gy = G.nodes[goal]['x'], G.nodes[goal]['y']
            ax.scatter(sx, sy, c='green', s=200, label='Start', zorder=5)
            ax.scatter(gx, gy, c='red', s=200, marker='*', label='Goal', zorder=5)
            ax.set_title(title, fontsize=14, fontweight='bold')
            ax.legend()
    
    plot_on_ax(ax1, astar_path, 'blue', f"A* (Planning)\nOptimal Distance: {metrics['astar_dist']:.1f}m")
    
    rl_title = f"Q-Learning (Learned)\nDistance: {metrics['rl_dist']:.1f}m"
    if rl_path[-1] != goal: rl_title += " (Failed)"
    plot_on_ax(ax2, rl_path, 'orange', rl_title)
    
    plt.tight_layout()
    plt.show()

    # --- VISUALIZATION 2: DATA TABLE ---
    df = pd.DataFrame({
        'Metric': ['Method Type', 'Knowledge Needed', 'Computation Phase', 'Path Optimality', 'Distance (m)'],
        'A* Algorithm': ['Search/Planning', 'Full Map Coordinates', 'At Query Time', 'Guaranteed Optimal', f"{metrics['astar_dist']:.1f}"],
        'Q-Learning': ['Reinforcement Learning', 'Local States Only', 'During Training', 'Approximate', f"{metrics['rl_dist']:.1f}"]
    })
    
    print("\n--- Detailed Comparison Table ---")
    display(df) if 'display' in globals() else print(df)
    
    # --- VISUALIZATION 3: EFFICIENCY CHART ---
    # Normalize values for easier charting
    categories = ['Path Efficiency', 'Compute Speed']
    
    # Calculate Efficiency (Lower distance is better)
    astar_eff = 100
    rl_eff = (metrics['astar_dist'] / metrics['rl_dist'] * 100) if metrics['rl_dist'] > 0 else 0
    
    fig, ax = plt.subplots(figsize=(8, 4))
    y_pos = np.arange(len(categories))
    width = 0.35
    
    # Note: Compute speed is inverted for visualization (A* is instant, RL takes time)
    # We just visualize Path Efficiency here for clarity
    plt.barh([0], [astar_eff], color='blue', height=0.3, label='A*')
    plt.barh([0], [rl_eff], color='orange', height=0.15, label='Q-Learning')
    
    plt.yticks([0], ['Path Optimality %'])
    plt.xlim(0, 110)
    plt.title("Algorithm Performance Comparison")
    plt.legend()
    plt.grid(axis='x', alpha=0.3)
    plt.show()

# Run the complete solution
run_experiment()

# ============================================================================
# PART 4: MULTI-PATH EVALUATION SUITE
# ============================================================================

def run_multi_path_experiment():
    # 1. Setup Map
    print("\nðŸ“ Downloading Map Data (Jerusalem)...")
    place = "Ramat Sharet, Jerusalem, Israel"
    G = ox.graph_from_address(place, dist=800, network_type='walk')
    nodes = list(G.nodes)
    
    # 2. Define Scenarios (Start Node Index, Goal Node Index, Label)
    scenarios = [
        (10, 50, "Short Scenario"),
        (0, 120, "Medium Scenario"),
        (40, 200, "Long Scenario")
    ]
    
    all_metrics = []

    for start_idx, goal_idx, label in scenarios:
        start_node = nodes[start_idx]
        goal_node = nodes[goal_idx]
        
        print(f"\n" + "="*40)
        print(f"RUNNING: {label}")
        print("="*40)

        # --- A* Baseline ---
        astar = AStarSolver(G)
        astar_path, astar_dist = astar.solve(start_node, goal_node)
        
        # --- Q-Learning ---
        env = NavigationEnvironment(G, start_node, goal_node)
        # We increase episodes for longer paths
        episodes = 800 if "Short" in label else 1500
        rl_agent = QLearningNavigator(env, alpha=0.1, gamma=0.98, epsilon=1.0)
        rl_agent.train(episodes=episodes)
        
        rl_path = rl_agent.get_path()
        
        # Calculate RL distance
        rl_dist = 0
        success = rl_path[-1] == goal_node
        if success:
            for i in range(len(rl_path)-1):
                rl_dist += G.get_edge_data(rl_path[i], rl_path[i+1])[0]['length']
        
        # Store Data
        all_metrics.append({
            'Scenario': label,
            'A* Dist (m)': round(astar_dist, 2),
            'RL Dist (m)': round(rl_dist, 2) if success else "FAILED",
            'Efficiency': f"{(astar_dist/rl_dist*100):.1f}%" if (success and rl_dist > 0) else "0%"
        })

        # --- Visualization for this specific path ---
        fig, ax = plt.subplots(figsize=(8, 6))
        ox.plot_graph(G, ax=ax, show=False, close=False, node_size=0, edge_color='#ddd')
        
        # Plot A*
        ax.plot([G.nodes[n]['x'] for n in astar_path], [G.nodes[n]['y'] for n in astar_path], 
                color='blue', linewidth=2, label='A* (Optimal)', alpha=0.6)
        
        # Plot RL
        if success:
            ax.plot([G.nodes[n]['x'] for n in rl_path], [G.nodes[n]['y'] for n in rl_path], 
                    color='orange', linewidth=4, label='Q-Learning', alpha=0.8)
        
        ax.set_title(f"Comparison: {label}")
        ax.legend()
        plt.show()

    # 3. Final Summary Table
    summary_df = pd.DataFrame(all_metrics)
    print("\n" + "!"*40)
    print("FINAL MULTI-PATH SUMMARY")
    print("!"*40)
    display(summary_df)

# Run the experiment
run_multi_path_experiment()
